# =================================================================
# MS SQL to PostgreSQL CDC Configuration
# =================================================================
# Copy this file to .env and update with your actual values
# DO NOT commit .env to version control (contains sensitive data)

# -----------------------------------------------------------------
# MS SQL Server Source Configuration
# -----------------------------------------------------------------
MSSQL_HOST=host.docker.internal       # Use 'host.docker.internal' for local Docker
MSSQL_PORT=1433                       # Default MS SQL port
MSSQL_USER=sa                         # MS SQL username
MSSQL_PASSWORD=YourStrong@Passw0rd    # MS SQL password (change this!)
MSSQL_DATABASE=Employees              # Source database name

# -----------------------------------------------------------------
# PostgreSQL Target Configuration
# -----------------------------------------------------------------
POSTGRES_HOST=host.docker.internal    # Use 'host.docker.internal' for local Docker
POSTGRES_PORT=5432                    # Default PostgreSQL port
POSTGRES_USER=postgres                # PostgreSQL username
POSTGRES_PASSWORD=postgres            # PostgreSQL password (change this!)
POSTGRES_DATABASE=target_db           # Target database name
POSTGRES_SCHEMA=dbo                   # Target schema name

# -----------------------------------------------------------------
# Kafka & Debezium Configuration
# -----------------------------------------------------------------
KAFKA_BOOTSTRAP_SERVERS=kafka:9092    # Kafka bootstrap servers (internal Docker network)
TOPIC_PREFIX=mssql                    # Kafka topic prefix for CDC events

# Connector Names
SOURCE_CONNECTOR_NAME=mssql-source-connector
SINK_CONNECTOR_NAME=postgres-sink-connector

# Connector Settings
CONNECTOR_TASKS_MAX=10                # Maximum parallel tasks per connector

# -----------------------------------------------------------------
# Data Type Transformation Settings
# -----------------------------------------------------------------
# Comma-separated list of columns to convert from string to UUID
UUID_COLUMNS=user_id,session_id,transaction_id,last_login_id,unique_identifier_col,customer_guid,product_guid

# Comma-separated list of columns to convert from string to JSON
JSON_COLUMNS=user_preferences,api_response,metadata,settings,preferences_json,order_metadata

# -----------------------------------------------------------------
# Performance & Memory Settings
# -----------------------------------------------------------------
# Kafka Connect JVM heap size (adjust based on your system)
KAFKA_HEAP_OPTS="-Xms2G -Xmx4G"

# Connection timeouts (milliseconds)
CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS=30000
CONNECT_OFFSET_FLUSH_INTERVAL_MS=60000

# =================================================================
# IMPORTANT NOTES:
# =================================================================
# 1. After copying to .env, update all passwords and sensitive data
# 2. For production: Use strong passwords and secure credential storage
# 3. For different environments: Create separate .env files (.env.dev, .env.prod)
# 4. host.docker.internal works on Docker Desktop (Windows/Mac) and Linux with extra_hosts
# 5. Add more UUID/JSON columns as needed for your schema
