# MS SQL to PostgreSQL CDC with Automatic Schema Replication

**Fully automatic Change Data Capture (CDC)** system that replicates data from MS SQL Server to PostgreSQL in real-time with:

- âœ… **Automatic schema replication** - No manual table creation!
- âœ… **VARCHAR length preservation** - VARCHAR(50) stays VARCHAR(50)
- âœ… **Native type conversion** - UNIQUEIDENTIFIER â†’ UUID, JSON â†’ JSON
- âœ… **Snake_case transformation** - FirstName â†’ first_name
- âœ… **Real-time replication** - 5-10 second latency
- âœ… **Soft delete tracking** - Deleted records marked, not removed
- âœ… **Cross-platform** - Works on Windows, Linux, macOS
- âœ… **Scales to 500+ tables**

---

## ğŸš€ Quick Start (5 Minutes)

### Option 1: Automated Deployment (Recommended)

```bash
# Clone and deploy everything with one script
git clone https://github.com/faizakram/mysql-cdc-go-single-table.git
cd mysql-cdc-go-single-table/debezium-setup
./scripts/deploy-all.sh
```

**This automated script:**
- âœ… Checks prerequisites
- âœ… Starts all infrastructure
- âœ… Sets up databases with CDC
- âœ… Replicates schema automatically
- âœ… Deploys connectors
- âœ… Verifies everything works

ğŸ‘‰ **Details:** See [DEPLOY_ALL_GUIDE.md](DEPLOY_ALL_GUIDE.md)

### Option 2: Manual Deployment

ğŸ‘‰ **New user?** See [QUICK_START.md](QUICK_START.md) to get started in 5 minutes.

ğŸ‘‰ **Complete instructions?** See [COMPLETE_INSTALLATION_GUIDE.md](COMPLETE_INSTALLATION_GUIDE.md) for step-by-step installation on Windows/Linux/macOS.

---

## ğŸ“– Documentation

| Guide | Description | Who Should Read |
|-------|-------------|-----------------|
| **[DEPLOY_ALL_GUIDE.md](DEPLOY_ALL_GUIDE.md)** | **Automated deployment script** | **Everyone (easiest!)** |
| **[QUICK_START.md](QUICK_START.md)** | Get started in 5 minutes | Manual deployment |
| **[COMPLETE_INSTALLATION_GUIDE.md](COMPLETE_INSTALLATION_GUIDE.md)** | Full installation guide (Windows/Linux/macOS) | New users |
| **[AUTOMATIC_SCHEMA_REPLICATION.md](AUTOMATIC_SCHEMA_REPLICATION.md)** | Technical details of automatic schema replication | Developers |
| **[SETUP_GUIDE.md](SETUP_GUIDE.md)** | Detailed configuration guide | Advanced users |
| **[TESTING_GUIDE.md](TESTING_GUIDE.md)** | Testing and validation procedures | QA engineers |

---

## Table of Contents
1. [Key Features](#key-features)
2. [Architecture Overview](#architecture-overview)
3. [Prerequisites](#prerequisites)
4. [Quick Installation](#quick-installation)
5. [Configuration](#configuration)
6. [Usage](#usage)
7. [Testing](#testing)
8. [Troubleshooting](#troubleshooting)
9. [Scaling to 500+ Tables](#scaling-to-500-tables)

---

## Key Features

### ğŸ¯ Automatic Schema Replication

**No more manual CREATE TABLE statements!**

The system automatically:
1. Reads MS SQL table schemas from `sys.columns`
2. Converts data types with full fidelity
3. Preserves VARCHAR lengths, UUID types, DECIMAL precision
4. Creates PostgreSQL tables automatically
5. Handles snake_case transformation

| MS SQL Type | PostgreSQL Type | Preserved |
|-------------|----------------|-----------|
| `NVARCHAR(50)` | `VARCHAR(50)` | âœ… Length |
| `UNIQUEIDENTIFIER` | `UUID` | âœ… Native type |
| `DECIMAL(10,2)` | `NUMERIC(10,2)` | âœ… Precision |
| `DATETIME2(6)` | `TIMESTAMP(6)` | âœ… Microseconds |
| `BIT` | `BOOLEAN` | âœ… Native type |

### ğŸ”„ Real-Time CDC

- **Latency**: 5-10 seconds
- **Operations**: INSERT, UPDATE, DELETE
- **Throughput**: 10,000+ events/second
- **Reliability**: Exactly-once delivery

### ğŸ Snake Case Transformation

Column names automatically transformed:
```
FirstName    â†’ first_name
EmailAddress â†’ email_address
IsActive     â†’ is_active
```

### ğŸ—‘ï¸ Soft Delete Tracking

Deleted records marked with `__cdc_deleted = 'true'`:
- History preserved
- Audit trail maintained
- Can restore if needed

---

## Table of Contents (Detailed)
1. [Architecture Overview](#architecture-overview)
2. [Prerequisites](#prerequisites)
3. [Quick Installation](#quick-installation)
4. [Configuration](#configuration)
5. [Usage](#usage)
6. [Testing](#testing)
7. [Monitoring & Verification](#monitoring--verification)
8. [Troubleshooting](#troubleshooting)
9. [Scaling to 500+ Tables](#scaling-to-500-tables)

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MS SQL Server      â”‚
â”‚ (Source: 500 tables)â”‚
â”‚  - CDC Enabled      â”‚
â”‚  - SQL Agent Runningâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Reads CDC tables
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Debezium Connect    â”‚
â”‚ SQL Server Connectorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Publishes to Kafka
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Kafka       â”‚
â”‚  (500 topics)       â”‚
â”‚  1 topic per table  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Consumes from Kafka
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Debezium Connect    â”‚
â”‚ PostgreSQL Connectorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Writes to target
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL         â”‚
â”‚ (Target: 500 tables)â”‚
â”‚  - Auto-created     â”‚
â”‚  - Real-time sync   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- âœ… Full initial snapshot (historical data)
- âœ… Real-time CDC for INSERT, UPDATE, DELETE
- âœ… Auto-table creation in PostgreSQL
- âœ… **PascalCase to snake_case transformation** (MS SQL â†’ PostgreSQL naming standards)
- âœ… Schema evolution support
- âœ… Sub-second replication latency
- âœ… Soft delete support with audit trail

---

## Prerequisites

### 1. System Requirements

**Server Resources:**
- **CPU**: 4+ cores (8+ recommended for 500 tables)
- **RAM**: 8GB minimum (16GB+ recommended)
- **Disk**: 50GB+ free space (for Kafka logs)
- **Network**: Low latency between MS SQL and PostgreSQL (<50ms ideal)

**Software Requirements:**
- Docker Engine 20.10+
- Docker Compose v2.0+
- MS SQL Server 2016+ (2019+ recommended)
- PostgreSQL 12+ (18 recommended)

### 2. MS SQL Server Prerequisites

#### 2.1 Enable SQL Server Agent (CRITICAL)

**SQL Server Agent MUST be running for CDC to work!**

**For Docker MS SQL:**
```bash
docker run -d --name mssql \
  -e 'ACCEPT_EULA=Y' \
  -e 'SA_PASSWORD=YourStrong!Passw0rd' \
  -e 'MSSQL_AGENT_ENABLED=true' \
  -e 'MSSQL_PID=Developer' \
  -p 1433:1433 \
  mcr.microsoft.com/mssql/server:2019-latest
```

**Verify SQL Server Agent is running:**
```sql
EXEC xp_servicecontrol 'QueryState', N'SQLServerAGENT';
-- Should return: Running.
```

#### 2.2 Enable CDC on Database

```sql
-- Switch to your database
USE YourDatabaseName;

-- Set recovery model to FULL (required for CDC)
ALTER DATABASE YourDatabaseName SET RECOVERY FULL;

-- Enable CDC on database
EXEC sys.sp_cdc_enable_db;

-- Verify CDC is enabled
SELECT name, is_cdc_enabled 
FROM sys.databases 
WHERE name = 'YourDatabaseName';
-- Should return: is_cdc_enabled = 1
```

#### 2.3 Enable CDC on Tables

**For ALL tables (recommended for 500 tables):**
```sql
USE YourDatabaseName;

-- Generate enable script for all tables in dbo schema
SELECT 
    'EXEC sys.sp_cdc_enable_table 
    @source_schema = ''dbo'',
    @source_name = ''' + name + ''',
    @role_name = NULL,
    @supports_net_changes = 1;'
FROM sys.tables
WHERE is_ms_shipped = 0  -- Exclude system tables
  AND schema_id = SCHEMA_ID('dbo')
ORDER BY name;
```

Copy the output and execute it to enable CDC on all tables.

**For specific tables:**
```sql
EXEC sys.sp_cdc_enable_table
    @source_schema = 'dbo',
    @source_name = 'Employees',
    @role_name = NULL,
    @supports_net_changes = 1;
```

**Verify CDC on tables:**
```sql
-- Check CDC-enabled tables
EXEC sys.sp_cdc_help_change_data_capture;

-- Should show all your tables with capture_instance names
```

#### 2.4 Create Debezium User (Optional but Recommended)

```sql
-- Create login with strong password
CREATE LOGIN debezium WITH PASSWORD = 'Debezium@Strong123!';

-- Create user in your database
USE YourDatabaseName;
CREATE USER debezium FOR LOGIN debezium;

-- Grant required permissions
GRANT SELECT ON SCHEMA::dbo TO debezium;
GRANT SELECT ON SCHEMA::cdc TO debezium;
GRANT EXECUTE ON SCHEMA::cdc TO debezium;
GRANT VIEW DATABASE STATE TO debezium;
```

### 3. PostgreSQL Prerequisites

#### 3.1 Create Target Database and Schema

```sql
-- Connect as postgres superuser
CREATE DATABASE target_db;

\c target_db;

-- Create dbo schema (matching MS SQL schema)
CREATE SCHEMA dbo;

-- Grant permissions to admin user
GRANT ALL ON SCHEMA dbo TO admin;
GRANT ALL ON ALL TABLES IN SCHEMA dbo TO admin;
ALTER DEFAULT PRIVILEGES IN SCHEMA dbo GRANT ALL ON TABLES TO admin;
```

### 4. Network Prerequisites

**Ensure connectivity between components:**

```bash
# From Debezium server, test MS SQL connection
telnet mssql-host 1433

# Test PostgreSQL connection
telnet postgres-host 5432

# Verify Docker network (if using Docker)
docker network ls
docker network inspect debezium-network
```

---

## Naming Convention Transformation

### Overview

This solution automatically transforms MS SQL Server's **PascalCase/camelCase** naming to PostgreSQL's **snake_case** standard:

**MS SQL Server:**
```sql
-- Tables: EmployeeData, OrderHistory, UserSettings
-- Columns: EmployeeID, FirstName, LastName, CreatedAt
```

**PostgreSQL (Transformed):**
```sql
-- Tables: employee_data, order_history, user_settings
-- Columns: employee_id, first_name, last_name, created_at
```

### How It Works

**Custom Kafka Connect Single Message Transform (SMT)**

The solution includes a custom Java SMT (`SnakeCaseTransform`) that:

1. **Transforms Topic Names** â†’ Table names (e.g., `EmployeeData` â†’ `employee_data`)
2. **Transforms Field Names** â†’ Column names (e.g., `FirstName` â†’ `first_name`)
3. **Handles Nested Structures** â†’ Recursively transforms all nested fields
4. **Preserves Data Integrity** â†’ Maintains all data types and constraints

**Transformation Rules:**
- `EmployeeID` â†’ `employee_id` (handles acronyms)
- `firstName` â†’ `first_name` (camelCase)
- `HTTPSConnection` â†’ `https_connection` (multiple capitals)
- `userName123` â†’ `user_name123` (with numbers)

### Custom SMT Components

**Files:**
- `custom-smt/pom.xml` - Maven build configuration
- `custom-smt/src/main/java/com/debezium/transforms/SnakeCaseTransform.java` - Transform implementation
- `deploy-smt.sh` - Automated build and deployment script

**Build & Deploy:**
```bash
cd debezium-setup
./deploy-smt.sh
```

This automatically:
1. Compiles the Java SMT using Maven
2. Creates an uber JAR with all dependencies
3. Copies JAR to Debezium Connect container
4. Restarts Debezium Connect to load the plugin

### Configuration

The SMT is configured in `connectors/postgres-sink.json`:

```json
{
  "transforms": "route,unwrap,snakeCaseKey,snakeCaseValue",
  
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.unwrap.drop.tombstones": "false",
  "transforms.unwrap.delete.handling.mode": "rewrite",
  
  "transforms.snakeCaseKey.type": "com.debezium.transforms.SnakeCaseTransform$Key",
  "transforms.snakeCaseValue.type": "com.debezium.transforms.SnakeCaseTransform$Value"
}
```

**Transform Chain:**
1. **RegexRouter** - Simplifies topic names (removes database prefix)
2. **ExtractNewRecordState** - Unwraps Debezium envelope, extracts `after` state
3. **SnakeCaseTransform$Key** - Transforms key field names
4. **SnakeCaseTransform$Value** - Transforms value field names

### Verification

**Check Table Names:**
```sql
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'dbo' 
ORDER BY table_name;

-- Expected: employees, order_history, user_settings (all lowercase)
```

**Check Column Names:**
```sql
SELECT column_name 
FROM information_schema.columns 
WHERE table_schema = 'dbo' 
  AND table_name = 'employees' 
ORDER BY ordinal_position;

-- Expected: employee_id, first_name, last_name (snake_case)
```

**Query Without Quotes:**
```sql
-- Works without quotes (PostgreSQL convention)
SELECT employee_id, first_name, last_name 
FROM dbo.employees 
WHERE employee_id > 1000;
```

### Benefits

âœ… **Consistency** - All tables follow PostgreSQL naming conventions  
âœ… **Maintainability** - No manual schema mappings required  
âœ… **Scalability** - Works automatically for all 500 tables  
âœ… **No Code Changes** - Application queries work naturally with snake_case  
âœ… **CDC Compatible** - Transformations applied in real-time for all operations

---

## Step-by-Step Setup

### Step 1: Configure Source Connector (MS SQL Server)

Edit `connectors/mssql-source.json` with your MS SQL credentials:

```json
{
  "name": "mssql-source-connector",
  "config": {
    "connector.class": "io.debezium.connector.sqlserver.SqlServerConnector",
    "tasks.max": "10",
    
    "database.hostname": "your-mssql-host.com",
    "database.port": "1433",
    "database.user": "debezium",
    "database.password": "Debezium@Strong123!",
    "database.names": "YourDatabaseName",
    "table.include.list": "dbo.*"
  }
}
```

### Step 2: Configure Sink Connector (PostgreSQL)

Edit `connectors/postgres-sink.json` with your PostgreSQL credentials:

```json
{
  "name": "postgres-sink-connector",
  "config": {
    "connector.class": "io.debezium.connector.jdbc.JdbcSinkConnector",
    "tasks.max": "10",
    
    "connection.url": "jdbc:postgresql://your-postgres-host:5432/target_db?currentSchema=dbo",
    "connection.username": "admin",
    "connection.password": "your-password"
  }
}
```

### Step 3: Deploy the Infrastructure

```bash
cd debezium-setup

# Make scripts executable
chmod +x setup.sh status.sh

# Start everything (Kafka, Zookeeper, Debezium, connectors)
./setup.sh
```

**What `setup.sh` does:**

1. Starts Kafka cluster + Zookeeper
2. Starts Debezium Connect service
3. Waits for services to be healthy
4. Deploys MS SQL source connector
5. Deploys PostgreSQL sink connector

**Expected output:**

```
âœ… Kafka started
âœ… Zookeeper started
âœ… Debezium Connect started
âœ… MS SQL connector deployed
âœ… PostgreSQL connector deployed
Setup complete!
```

### Step 4: Verify Deployment

```bash
# Check connector status
./status.sh
```

**Expected output:**

```json
{
  "name": "mssql-source-connector",
  "connector": {"state": "RUNNING"},
  "tasks": [{"id": 0, "state": "RUNNING"}]
}

{
  "name": "postgres-sink-connector",
  "connector": {"state": "RUNNING"},
  "tasks": [{"id": 0, "state": "RUNNING"}]
}
```

---

## Monitoring & Verification

### Real-Time Monitoring URLs

#### 1. **Kafka UI** (Primary Dashboard)

**URL:** `http://localhost:8080` or `http://your-server-ip:8080`

**What you can see:**

- ğŸ“Š **Topics**: All 500 Kafka topics (one per table)
- ğŸ“ˆ **Messages**: Real-time message flow
- ğŸ” **Brokers**: Kafka cluster health
- ğŸ‘¥ **Consumer Groups**: Connector consumption status
- ğŸ“ **Message Browser**: View actual CDC events

**Key Metrics to Watch:**

- **Topic list**: Should show `mssql.YourDB.dbo.TableName` for each table
- **Message count**: Increases as data replicates
- **Consumer lag**: Should be near 0 for real-time replication

#### 2. **Debezium Connect REST API**

**Base URL:** `http://localhost:8083` or `http://your-server-ip:8083`

**Useful Endpoints:**

```bash
# List all connectors
curl http://localhost:8083/connectors | jq

# Check MS SQL connector status
curl http://localhost:8083/connectors/mssql-source-connector/status | jq

# Check PostgreSQL connector status
curl http://localhost:8083/connectors/postgres-sink-connector/status | jq

# Get connector configuration
curl http://localhost:8083/connectors/mssql-source-connector/config | jq

# View recent connector errors
curl http://localhost:8083/connectors/mssql-source-connector/status | jq '.tasks[].trace'
```

#### 3. **Docker Logs**

**View real-time logs:**

```bash
# All services
docker compose logs -f

# Debezium Connect only
docker compose logs -f debezium-connect

# Kafka only
docker compose logs -f kafka

# Last 100 lines
docker compose logs --tail=100 debezium-connect
```

**Search for errors:**

```bash
# Find errors in last hour
docker compose logs --since 1h debezium-connect | grep -i error

# Find specific connector logs
docker compose logs debezium-connect | grep "mssql-source-connector"
```

### Verification Queries

#### Check Replication Progress

**MS SQL (Source):**

```sql
-- Count rows in source table
USE YourDatabaseName;
SELECT COUNT(*) FROM dbo.Employees;
```

**PostgreSQL (Target):**

```sql
-- Count rows in target table
\c target_db;
SELECT COUNT(*) FROM dbo."Employees";
```

**Compare counts:**

```bash
# Quick comparison script
echo "MS SQL Count:"
docker exec mssql-test /opt/mssql-tools18/bin/sqlcmd -S localhost -U SA -P 'YourStrong!Passw0rd' -C -Q "USE YourDB; SELECT COUNT(*) FROM dbo.Employees;"

echo "PostgreSQL Count:"
docker exec postgres18 psql -U admin -d target_db -c 'SELECT COUNT(*) FROM dbo."Employees";'
```

#### Monitor CDC Lag

```bash
# Check Kafka consumer lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --group connect-postgres-sink-connector

# Look for LAG column - should be 0 or very small
```

#### Test Real-Time CDC

```sql
-- MS SQL: Insert test row
USE YourDatabaseName;
INSERT INTO dbo.Employees (firstName, lastName, salary) 
VALUES ('Test', 'CDC', 99999.99);

-- Wait 2-3 seconds, then check PostgreSQL
```

```sql
-- PostgreSQL: Verify row appeared
SELECT * FROM dbo."Employees" 
WHERE "firstName" = 'Test' AND "lastName" = 'CDC';
-- Should show the new row within seconds!
```

---

## Expected Timeline

### Phase 1: Initial Snapshot (Full Load)

**For 500 tables with 15M rows each:**

- **Total rows**: 7.5 billion rows
- **Debezium performance**: 100K-500K rows/second (depends on hardware)
- **Estimated time**: 
  - Fast setup (500K rows/sec): ~4 hours
  - Medium setup (250K rows/sec): ~8 hours  
  - Slow setup (100K rows/sec): ~20 hours

**Progress indicators:**

```bash
# Watch snapshot progress
docker compose logs -f debezium-connect | grep "Finished exporting"

# Example output:
# Finished exporting 15000000 records for table 'YourDB.dbo.Employees' (1 of 500 tables)
# Finished exporting 15000000 records for table 'YourDB.dbo.Orders' (2 of 500 tables)
```

### Phase 2: Real-Time CDC (After Snapshot)

- **Replication latency**: <1 second
- **INSERT/UPDATE/DELETE**: Captured and replicated in real-time
- **No downtime required**: Application can continue writing to MS SQL

---

## Troubleshooting

### Common Issues

#### 1. Connector Status: FAILED

**Check error details:**

```bash
curl http://localhost:8083/connectors/mssql-source-connector/status | jq '.tasks[].trace'
```

**Common causes:**

- âŒ SQL Server Agent not running
- âŒ CDC not enabled on database/tables
- âŒ Incorrect credentials
- âŒ Network connectivity issues

**Solution:**

```sql
-- Verify SQL Server Agent
EXEC xp_servicecontrol 'QueryState', N'SQLServerAGENT';
-- Must return: Running.

-- Verify CDC enabled
SELECT name, is_cdc_enabled FROM sys.databases WHERE name = 'YourDB';
-- is_cdc_enabled must be 1

-- Check CDC tables
EXEC sys.sp_cdc_help_change_data_capture;
-- Should list all your tables
```

#### 2. Tables Not Created in PostgreSQL

**Symptom**: Kafka topics have data but PostgreSQL tables are empty or don't exist.

**Diagnosis:**

```bash
# Check if Kafka topics have messages
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic mssql.YourDB.dbo.Employees \
  --from-beginning \
  --max-messages 1
```

**Solutions:**

```bash
# Option 1: Reset sink connector consumer group
curl -X DELETE http://localhost:8083/connectors/postgres-sink-connector
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --delete \
  --group connect-postgres-sink-connector

# Redeploy sink connector
curl -X POST -H "Content-Type: application/json" \
  --data @connectors/postgres-sink.json \
  http://localhost:8083/connectors
```

#### 3. Slow Replication / High Lag

**Check consumer lag:**

```bash
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --group connect-postgres-sink-connector
```

**Optimization:**

1. **Increase parallel tasks** in `postgres-sink.json`:

```json
{
  "tasks.max": "20"  // Increase from 10 to 20
}
```

2. **Increase batch size** (add to postgres-sink.json):

```json
{
  "batch.size": "1000",
  "consumer.max.poll.records": "1000"
}
```

3. **Scale PostgreSQL writes**:

```sql
-- Increase PostgreSQL performance
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '4GB';
ALTER SYSTEM SET effective_cache_size = '12GB';
SELECT pg_reload_conf();
```

#### 4. Kafka Disk Space Issues

**Check disk usage:**

```bash
docker exec kafka df -h /kafka

# Clean old logs (if needed)
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics \
  --entity-name mssql.YourDB.dbo.Employees \
  --alter \
  --add-config retention.ms=86400000  # 1 day retention
```

#### 5. Connector Keeps Restarting

**Check Debezium logs:**

```bash
docker compose logs debezium-connect --tail=200 | grep -i error
```

**Common fixes:**

```bash
# Restart Debezium Connect
docker compose restart debezium-connect

# If that doesn't work, full reset
docker compose down
docker compose up -d
./setup.sh
```

### Logging & Debugging

#### Enable Verbose Logging

Edit `docker-compose.yml` to add:

```yaml
debezium-connect:
  environment:
    - LOG_LEVEL=DEBUG
    - CONNECT_LOG_LEVEL=DEBUG
```

Restart:

```bash
docker compose down
docker compose up -d
```

#### View Connector-Specific Logs

```bash
# MS SQL connector logs
docker compose logs debezium-connect | grep "SQL_Server"

# PostgreSQL connector logs  
docker compose logs debezium-connect | grep "postgres-sink"
```

#### Export Logs for Support

```bash
# Save last 1000 lines to file
docker compose logs --tail=1000 debezium-connect > debezium-logs.txt

# Save specific time range
docker compose logs --since "2025-12-09T10:00:00" --until "2025-12-09T12:00:00" > incident-logs.txt
```

---

## Scaling to 500 Tables

### Automatic Discovery

Debezium will **automatically discover all CDC-enabled tables** in your database. No code changes needed!

Just ensure CDC is enabled on all 500 tables (see [Prerequisites](#23-enable-cdc-on-tables)).

### Performance Tuning for 500 Tables

#### 1. Increase Connector Tasks

```json
// mssql-source.json
{
  "tasks.max": "10"  // Can go up to 20-30 for large deployments
}

// postgres-sink.json  
{
  "tasks.max": "20"  // More tasks = faster writes
}
```

#### 2. Kafka Configuration

Edit `docker-compose.yml`:

```yaml
kafka:
  environment:
    - KAFKA_NUM_PARTITIONS=3  # More partitions = better parallelism
    - KAFKA_LOG_RETENTION_HOURS=24  # Reduce to save disk space
    - KAFKA_LOG_SEGMENT_BYTES=1073741824  # 1GB segments
```

#### 3. Resource Allocation

```yaml
debezium-connect:
  deploy:
    resources:
      limits:
        cpus: '8'
        memory: 16G
      reservations:
        cpus: '4'
        memory: 8G
```

### Monitoring 500 Tables

```bash
# Count topics created (should be ~500)
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092 | grep "^mssql\." | wc -l

# Check total messages across all topics
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic "mssql.*" \
  | awk -F: '{sum += $3} END {print "Total messages:", sum}'
```

---

## Production Deployment Checklist

Before going to production with 500 tables:

- [ ] SQL Server Agent is running
- [ ] CDC enabled on all 500 tables (verify with `sp_cdc_help_change_data_capture`)
- [ ] Network latency < 50ms between MS SQL and PostgreSQL
- [ ] Sufficient disk space (100GB+ recommended for Kafka logs)
- [ ] Monitoring dashboards configured (Kafka UI accessible)
- [ ] Backup strategy for PostgreSQL target database
- [ ] Alerts configured for connector failures
- [ ] Test CDC works: INSERT/UPDATE/DELETE all replicate
- [ ] Verify initial snapshot completed for test tables
- [ ] Document rollback procedure
- [ ] Performance testing completed

---

## Quick Reference

### Essential URLs

| Service | URL | Purpose |
|---------|-----|---------|
| **Kafka UI** | `http://localhost:8080` | Visual monitoring, topic browser |
| **Debezium API** | `http://localhost:8083` | Connector status and management |
| **Kafka Broker** | `localhost:9092` | Internal Kafka access |

### Essential Commands

```bash
# Start everything
./setup.sh

# Check status
./status.sh

# Stop everything
docker compose down

# Restart connectors
curl -X POST http://localhost:8083/connectors/mssql-source-connector/restart
curl -X POST http://localhost:8083/connectors/postgres-sink-connector/restart

# View logs
docker compose logs -f debezium-connect

# Test CDC
# (Insert row in MS SQL, check PostgreSQL after 2-3 seconds)
```

### Emergency Procedures

**If replication stops:**

```bash
# 1. Check connector status
./status.sh

# 2. Check for errors
docker compose logs debezium-connect | grep -i error | tail -20

# 3. Restart failed connector
curl -X POST http://localhost:8083/connectors/CONNECTOR_NAME/restart

# 4. If still failing, full restart
docker compose restart
./setup.sh
```

**If need to start fresh:**

```bash
# WARNING: This deletes all Kafka data and resets replication

# 1. Stop everything
docker compose down -v  # -v deletes volumes

# 2. Clean PostgreSQL target (optional)
docker exec postgres18 psql -U admin -d target_db -c "DROP SCHEMA dbo CASCADE; CREATE SCHEMA dbo;"

# 3. Start fresh
docker compose up -d
./setup.sh
```

---

## Support & Resources

### Official Documentation

- **Debezium SQL Server Connector**: https://debezium.io/documentation/reference/stable/connectors/sqlserver.html
- **Debezium JDBC Sink**: https://debezium.io/documentation/reference/stable/connectors/jdbc.html
- **Kafka Documentation**: https://kafka.apache.org/documentation/

### Performance Benchmarks

- **Small tables (< 1M rows)**: Snapshot in minutes
- **Medium tables (1M-10M rows)**: Snapshot in 1-2 hours  
- **Large tables (10M-50M rows)**: Snapshot in 4-8 hours
- **CDC latency**: Typically < 500ms end-to-end

### Architecture Decisions

- **Why Kafka?**: Decouples source and sink, provides replay capability
- **Why Debezium?**: Enterprise-grade, battle-tested, schema evolution support
- **Why not AWS DMS?**: Vendor lock-in, limited schema evolution, higher cost
- **Why not custom code?**: 3-6 months development vs 2 days with Debezium

---

**Last Updated**: December 9, 2025  
**Version**: 1.0  
**Tested With**: Debezium 2.5, Kafka 7.5.0, MS SQL Server 2019, PostgreSQL 18
- **Changes**: INSERT/UPDATE/DELETE captured instantly

## Monitoring

### 1. Kafka UI (Best for visual monitoring)
```
http://localhost:8080
```
- See all 500 topics
- Message counts per topic
- Consumer lag

### 2. Debezium API
```bash
# List connectors
curl http://localhost:8083/connectors

# Check status
curl http://localhost:8083/connectors/mssql-source-connector/status | jq

# Check tasks
curl http://localhost:8083/connectors/mssql-source-connector/tasks | jq
```

### 3. PostgreSQL Progress
```sql
-- Check table counts
SELECT 
    schemaname,
    tablename,
    n_live_tup as row_count
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC;

-- Compare with source
SELECT COUNT(*) FROM source_table;  -- MS SQL
SELECT COUNT(*) FROM target_table;  -- PostgreSQL
```

## Troubleshooting

### MS SQL CDC Not Working
```sql
-- Check if CDC is enabled
SELECT name, is_cdc_enabled FROM sys.databases;

-- Check CDC jobs
EXEC sys.sp_cdc_help_jobs;

-- Check CDC tables
SELECT * FROM cdc.change_tables;
```

### Connector Fails
```bash
# View detailed errors
docker-compose logs debezium-connect | grep ERROR

# Restart connector
curl -X POST http://localhost:8083/connectors/mssql-source-connector/restart

# Delete and recreate
curl -X DELETE http://localhost:8083/connectors/mssql-source-connector
./setup.sh
```

### Slow Performance
```bash
# Increase Debezium memory
# Edit docker-compose.yml:
KAFKA_HEAP_OPTS: "-Xms4G -Xmx8G"

# Increase tasks (parallel processing)
# Edit connectors/*.json:
"tasks.max": "20"

# Restart
docker-compose restart debezium-connect
```

## Type Mappings (Automatic)

Debezium handles these automatically:

| MS SQL Server | PostgreSQL | Notes |
|--------------|------------|-------|
| NVARCHAR | VARCHAR | âœ… Auto |
| VARCHAR | VARCHAR | âœ… Auto |
| INT | INTEGER | âœ… Auto |
| BIGINT | BIGINT | âœ… Auto |
| DECIMAL | NUMERIC | âœ… Auto |
| DATETIME2 | TIMESTAMP | âœ… Auto |
| BIT | BOOLEAN | âœ… Auto |
| UNIQUEIDENTIFIER | UUID | âœ… Auto |
| VARBINARY | BYTEA | âœ… Auto |

## Cost Estimate

**Hardware Requirements** (for 500 tables):
- Kafka: 4GB RAM, 2 CPU cores, 50GB disk
- Debezium: 4GB RAM, 2 CPU cores
- Total: 8GB RAM, 4 cores, 50GB disk

**Cloud Costs** (if self-hosting):
- AWS EC2 t3.xlarge: ~$120/month
- AWS EBS 50GB: ~$5/month
- **Total**: ~$125/month (only during migration)

**Alternative**: Run on existing server (just needs Docker)

## Useful Commands

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f

# Check status
./status.sh

# Restart connectors
curl -X POST http://localhost:8083/connectors/mssql-source-connector/restart
curl -X POST http://localhost:8083/connectors/postgres-sink-connector/restart

# Pause replication
curl -X PUT http://localhost:8083/connectors/mssql-source-connector/pause

# Resume replication
curl -X PUT http://localhost:8083/connectors/mssql-source-connector/resume

# Delete connector
curl -X DELETE http://localhost:8083/connectors/mssql-source-connector
```

## Next Steps After Setup

1. **Validate Data**: Compare row counts between MS SQL and PostgreSQL
2. **Test Application**: Point app to PostgreSQL, verify functionality
3. **Cutover Plan**: Schedule downtime for final sync
4. **Decommission**: Stop Debezium after successful migration

## Support

- Debezium Docs: https://debezium.io/documentation/
- MS SQL Connector: https://debezium.io/documentation/reference/connectors/sqlserver.html
- Community: https://debezium.zulipchat.com/
